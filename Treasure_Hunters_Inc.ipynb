{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treasure Hunters Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to use reinforcement learning in a grid environment to avoid obstacles and reach the goal. This is done defining cost and reward functions with **Markov Decision Processes (MDP)**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Recall the basic ideas of Reinforcement Learning:\n",
    "\n",
    "## Basic idea:\n",
    "\n",
    "* Receive feedback in the form of rewards\n",
    "* Agent’s utility is defined by the reward function\n",
    "* Must (learn to) act so as to maximize expected rewards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid World\n",
    "\n",
    "* The agent lives in a grid\n",
    "* Walls block the agent’s path\n",
    "* The agent’s actions do not always go as planned\n",
    "* Small rewards at each step\n",
    "* Big rewards come at the end\n",
    "* Goal: maximize sum of rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We will make use of **Markov Decision Processes**.\n",
    "\n",
    "## Markov Decision Processes\n",
    "\n",
    "\n",
    "* An MDP is defined by:\n",
    "    * A set of states s ∈ S\n",
    "    * A set of actions a ∈ A\n",
    "    * A **transition function** T(s,a,s’)\n",
    "    * A **reward function** R(s, a, s’)\n",
    "    * A **start state** (or distribution)\n",
    "    * A **terminal state** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import all the libraries that we will use in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define all the above functions and probabilities. In order to do that, we define a class  called **GridWorldMDP**. With it we can define a function of the dollowing parameters:\n",
    "\n",
    "* reward_grid,\n",
    "* terminal_mask,\n",
    "* obstacle_mask,\n",
    "* action_probabilities,\n",
    "* no_action_probability\n",
    "\n",
    "With them we run a **Markov Decision Processes** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldMDP:\n",
    "\n",
    "    # up, right, down, left\n",
    "    _direction_deltas = [\n",
    "        (-1, 0),\n",
    "        (0, 1),\n",
    "        (1, 0),\n",
    "        (0, -1),\n",
    "    ]\n",
    "    _num_actions = len(_direction_deltas)\n",
    "\n",
    "    def __init__(self,\n",
    "                 reward_grid,\n",
    "                 terminal_mask,\n",
    "                 obstacle_mask,\n",
    "                 action_probabilities,\n",
    "                 no_action_probability):\n",
    "\n",
    "        self._reward_grid = reward_grid\n",
    "        self._terminal_mask = terminal_mask\n",
    "        self._obstacle_mask = obstacle_mask\n",
    "        self._T = self._create_transition_matrix(\n",
    "            action_probabilities,\n",
    "            no_action_probability,\n",
    "            obstacle_mask\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._reward_grid.shape\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self._reward_grid.size\n",
    "\n",
    "    @property\n",
    "    def reward_grid(self):\n",
    "        return self._reward_grid\n",
    "\n",
    "    def run_value_iterations(self, discount=1.0,\n",
    "                             iterations=10):\n",
    "        utility_grids, policy_grids = self._init_utility_policy_storage(iterations)\n",
    "\n",
    "        utility_grid = np.zeros_like(self._reward_grid)\n",
    "        for i in range(iterations):\n",
    "            utility_grid = self._value_iteration(utility_grid=utility_grid)\n",
    "            policy_grids[:, :, i] = self.best_policy(utility_grid)\n",
    "            utility_grids[:, :, i] = utility_grid\n",
    "        return policy_grids, utility_grids\n",
    "\n",
    "    def run_policy_iterations(self, discount=1.0,\n",
    "                              iterations=10):\n",
    "        utility_grids, policy_grids = self._init_utility_policy_storage(iterations)\n",
    "\n",
    "        policy_grid = np.random.randint(0, self._num_actions,\n",
    "                                        self.shape)\n",
    "        utility_grid = self._reward_grid.copy()\n",
    "\n",
    "        for i in range(iterations):\n",
    "            policy_grid, utility_grid = self._policy_iteration(\n",
    "                policy_grid=policy_grid,\n",
    "                utility_grid=utility_grid\n",
    "            )\n",
    "            policy_grids[:, :, i] = policy_grid\n",
    "            utility_grids[:, :, i] = utility_grid\n",
    "        return policy_grids, utility_grids\n",
    "\n",
    "    def generate_experience(self, current_state_idx, action_idx):\n",
    "        sr, sc = self.grid_indices_to_coordinates(current_state_idx)\n",
    "        next_state_probs = self._T[sr, sc, action_idx, :, :].flatten()\n",
    "\n",
    "        next_state_idx = np.random.choice(np.arange(next_state_probs.size),\n",
    "                                          p=next_state_probs)\n",
    "\n",
    "        return (next_state_idx,\n",
    "                self._reward_grid.flatten()[next_state_idx],\n",
    "                self._terminal_mask.flatten()[next_state_idx])\n",
    "\n",
    "    def grid_indices_to_coordinates(self, indices=None):\n",
    "        if indices is None:\n",
    "            indices = np.arange(self.size)\n",
    "        return np.unravel_index(indices, self.shape)\n",
    "\n",
    "    def grid_coordinates_to_indices(self, coordinates=None):\n",
    "        # Annoyingly, this doesn't work for negative indices.\n",
    "        # The mode='wrap' parameter only works on positive indices.\n",
    "        if coordinates is None:\n",
    "            return np.arange(self.size)\n",
    "        return np.ravel_multi_index(coordinates, self.shape)\n",
    "\n",
    "    def best_policy(self, utility_grid):\n",
    "        M, N = self.shape\n",
    "        return np.argmax((utility_grid.reshape((1, 1, 1, M, N)) * self._T)\n",
    "                         .sum(axis=-1).sum(axis=-1), axis=2)\n",
    "\n",
    "    def _init_utility_policy_storage(self, depth):\n",
    "        M, N = self.shape\n",
    "        utility_grids = np.zeros((M, N, depth))\n",
    "        policy_grids = np.zeros_like(utility_grids)\n",
    "        return utility_grids, policy_grids\n",
    "\n",
    "    def _create_transition_matrix(self,\n",
    "                                  action_probabilities,\n",
    "                                  no_action_probability,\n",
    "                                  obstacle_mask):\n",
    "        M, N = self.shape\n",
    "\n",
    "        T = np.zeros((M, N, self._num_actions, M, N))\n",
    "\n",
    "        r0, c0 = self.grid_indices_to_coordinates()\n",
    "\n",
    "        T[r0, c0, :, r0, c0] += no_action_probability\n",
    "\n",
    "        for action in range(self._num_actions):\n",
    "            for offset, P in action_probabilities:\n",
    "                direction = (action + offset) % self._num_actions\n",
    "\n",
    "                dr, dc = self._direction_deltas[direction]\n",
    "                r1 = np.clip(r0 + dr, 0, M - 1)\n",
    "                c1 = np.clip(c0 + dc, 0, N - 1)\n",
    "\n",
    "                temp_mask = obstacle_mask[r1, c1].flatten()\n",
    "                r1[temp_mask] = r0[temp_mask]\n",
    "                c1[temp_mask] = c0[temp_mask]\n",
    "\n",
    "                T[r0, c0, action, r1, c1] += P\n",
    "\n",
    "        terminal_locs = np.where(self._terminal_mask.flatten())[0]\n",
    "        T[r0[terminal_locs], c0[terminal_locs], :, :, :] = 0\n",
    "        return T\n",
    "\n",
    "    def _value_iteration(self, utility_grid, discount=1.0):\n",
    "        out = np.zeros_like(utility_grid)\n",
    "        M, N = self.shape\n",
    "        for i in range(M):\n",
    "            for j in range(N):\n",
    "                out[i, j] = self._calculate_utility((i, j),\n",
    "                                                    discount,\n",
    "                                                    utility_grid)\n",
    "        return out\n",
    "\n",
    "    def _policy_iteration(self, *, utility_grid,\n",
    "                          policy_grid, discount=1.0):\n",
    "        r, c = self.grid_indices_to_coordinates()\n",
    "\n",
    "        M, N = self.shape\n",
    "\n",
    "        utility_grid = (\n",
    "            self._reward_grid +\n",
    "            discount * ((utility_grid.reshape((1, 1, 1, M, N)) * self._T)\n",
    "                        .sum(axis=-1).sum(axis=-1))[r, c, policy_grid.flatten()]\n",
    "            .reshape(self.shape)\n",
    "        )\n",
    "\n",
    "        utility_grid[self._terminal_mask] = self._reward_grid[self._terminal_mask]\n",
    "\n",
    "        return self.best_policy(utility_grid), utility_grid\n",
    "\n",
    "    def _calculate_utility(self, loc, discount, utility_grid):\n",
    "        if self._terminal_mask[loc]:\n",
    "            return self._reward_grid[loc]\n",
    "        row, col = loc\n",
    "        return np.max(\n",
    "            discount * np.sum(\n",
    "                np.sum(self._T[row, col, :, :, :] * utility_grid,\n",
    "                       axis=-1),\n",
    "                axis=-1)\n",
    "        ) + self._reward_grid[loc]\n",
    "\n",
    "    def plot_policy(self, utility_grid, policy_grid=None):\n",
    "        if policy_grid is None:\n",
    "            policy_grid = self.best_policy(utility_grid)\n",
    "        markers = \"^>v<\"\n",
    "        marker_size = 100 // np.max(policy_grid.shape)\n",
    "        marker_edge_width = marker_size // 10\n",
    "        marker_fill_color = 'w'\n",
    "\n",
    "        no_action_mask = self._terminal_mask | self._obstacle_mask\n",
    "\n",
    "        utility_normalized = (utility_grid - utility_grid.min()) / \\\n",
    "                             (utility_grid.max() - utility_grid.min())\n",
    "\n",
    "        utility_normalized = (255*utility_normalized).astype(np.uint8)\n",
    "\n",
    "        utility_rgb = cv2.applyColorMap(utility_normalized, cv2.COLORMAP_JET)\n",
    "        for i in range(3):\n",
    "            channel = utility_rgb[:, :, i]\n",
    "            channel[self._obstacle_mask] = 0\n",
    "\n",
    "        plt.imshow(utility_rgb[:, :, ::-1], interpolation='none')\n",
    "            \n",
    "        for i, marker in enumerate(markers):\n",
    "            y, x = np.where((policy_grid == i) & np.logical_not(no_action_mask))\n",
    "            plt.plot(x, y, marker, ms=marker_size, mew=marker_edge_width, color=marker_fill_color)\n",
    "\n",
    "        y, x = np.where(self._terminal_mask)\n",
    "        plt.plot(x, y, 'o', ms=marker_size, mew=marker_edge_width, color=marker_fill_color)\n",
    "\n",
    "        tick_step_options = np.array([1, 2, 5, 10, 20, 50, 100])\n",
    "        tick_step = np.max(policy_grid.shape)/8\n",
    "        best_option = np.argmin(np.abs(np.log(tick_step) - np.log(tick_step_options)))\n",
    "        tick_step = tick_step_options[best_option]\n",
    "        plt.xticks(np.arange(0, policy_grid.shape[1] - 0.5, tick_step))\n",
    "        plt.yticks(np.arange(0, policy_grid.shape[0] - 0.5, tick_step))\n",
    "        plt.xlim([-0.5, policy_grid.shape[0]-0.5])\n",
    "        plt.xlim([-0.5, policy_grid.shape[1]-0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our first toy example. To make it easier, let's use a 2x2 grid.\n",
    "\n",
    "The game starts at the left bottom corner.\n",
    "\n",
    "The end is the top right corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting a 3x3 game with One Obstacle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (3, 3)\n",
    "goal = (0, 2)\n",
    "trap = ((1, 1))\n",
    "obstacle = (1, 1)\n",
    "start = (-3, 0)\n",
    "default_reward = -0.1\n",
    "goal_reward = 2\n",
    "trap_reward = -1\n",
    "\n",
    "reward_grid = np.zeros(shape) + default_reward\n",
    "reward_grid[goal] = goal_reward\n",
    "reward_grid[trap] = trap_reward\n",
    "reward_grid[obstacle] = 1\n",
    "\n",
    "terminal_mask = np.zeros_like(reward_grid, dtype=np.bool)\n",
    "terminal_mask[goal] = True\n",
    "terminal_mask[trap] = False\n",
    "\n",
    "obstacle_mask = np.zeros_like(reward_grid, dtype=np.bool)\n",
    "obstacle_mask[1, 1] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we just want to show how the game look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1, -0.1,  2. ],\n",
       "       [-0.1,  1. , -0.1],\n",
       "       [-0.1, -0.1, -0.1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True],\n",
       "       [False, False, False],\n",
       "       [False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False],\n",
       "       [False,  True, False],\n",
       "       [False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obstacle_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x8501930>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAA/hJREFUeJzt17FNK1sUQNHLFxkp5M4ogCnA1VANr4FpBhfgAhwSOEEQQjy/A2M/gezNW0ua7AZHx96aO1fLsiwDSPnv3AMApxMuBAkXgoQLQcKFIOFCkHAhSLgQJFwIuj7l8O3t7VitVj80St/n5+e4ubk59xgXzY4Oe3l5Ge/v71+eOync1Wo1ttvtXw/12202m7Fer889xkWzo8OmaTrqnKsyBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIej6qwPzPI95nscYY+z3+7HZbH56pqzX19fx58+fc49x0e7v7/2HvsNygoeHh1OO/3Oenp6WMYbnwPP8/Hzun+miHduYqzIECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAh6PrcA/wmDw8PY1mWc49x0TabzblH+BW+DHee5zHP8xhjjP1+b/EHfHx82M8X7Oh7XC0nvCKmaRrb7fYn50nbbDZjvV6fe4yLZkeHHduYb1wIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFC0PVXB+Z5HvM8jzHG2O12Y5qmHx+q6u3tbdzd3Z17jItmR4ftdrujzl0ty7L88Cz/jGmaxna7PfcYF82ODjt2P67KECRcCBLuN3p8fDz3CBfPjg47dj++cSHIGxeChAtBwoUg4UKQcCHof8NATbIbU/bVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x84b7970>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.grid('on')\n",
    "nrows, ncols =shape\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.imshow(obstacle_mask*-1, cmap='gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = GridWorldMDP(reward_grid=reward_grid,\n",
    "                     obstacle_mask=obstacle_mask,\n",
    "                     terminal_mask=terminal_mask,\n",
    "                     action_probabilities=[\n",
    "                         (-1, 0.1),\n",
    "                         (0, 0.8),\n",
    "                         (1, 0.1),\n",
    "                     ],\n",
    "                     no_action_probability=0.0)\n",
    "mdp_solvers = {'Value Iteration': gw.run_value_iterations,\n",
    "                'Policy Iteration': gw.run_policy_iterations}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we print the results of our policy and utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final result of Value Iteration:\n",
      "[[ 1.  1.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "[[ 1.73437499  1.875       2.        ]\n",
      " [ 1.60937497  2.84843749  1.875     ]\n",
      " [ 1.49826381  1.60937497  1.73437499]]\n",
      "Final result of Policy Iteration:\n",
      "[[ 1.  1.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "[[ 1.73437499  1.875       2.        ]\n",
      " [ 1.60937497  2.84843749  1.875     ]\n",
      " [ 1.49826381  1.60937497  1.73437499]]\n"
     ]
    }
   ],
   "source": [
    "for solver_name, solver_fn in mdp_solvers.items():\n",
    "        print('Final result of {}:'.format(solver_name))\n",
    "        policy_grids, utility_grids = solver_fn(iterations=25, discount=0.5)\n",
    "        print(policy_grids[:, :, -1])\n",
    "        print(utility_grids[:, :, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the final result of this toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8501790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "ax = plt.gca()\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "gw.plot_policy(utility_grids[:, :, -1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, there are two possible solutions. Starting from the bottom, we can either start moving right or up and we will avoid the obstacle and reach the goal with the same number of steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting a 5x5 game with Two Obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (5, 5)\n",
    "goal = (0, 4)\n",
    "trap = ((1,1))\n",
    "obstacle1 = (1,1)\n",
    "obstacle2 = (3,3)\n",
    "start = (-4, 0)\n",
    "default_reward = -0.1\n",
    "goal_reward = 2\n",
    "trap_reward = -1\n",
    "\n",
    "reward_grid = np.zeros(shape) + default_reward\n",
    "reward_grid[goal] = goal_reward\n",
    "reward_grid[trap] = trap_reward\n",
    "reward_grid[obstacle1] = 1\n",
    "reward_grid[obstacle2] = 1\n",
    "\n",
    "terminal_mask = np.zeros_like(reward_grid, dtype=np.bool)\n",
    "terminal_mask[goal] = True\n",
    "terminal_mask[trap] = False\n",
    "\n",
    "obstacle_mask = np.zeros_like(reward_grid, dtype=np.bool)\n",
    "obstacle_mask[obstacle1] = True\n",
    "obstacle_mask[obstacle2] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1, -0.1, -0.1, -0.1,  2. ],\n",
       "       [-0.1,  1. , -0.1, -0.1, -0.1],\n",
       "       [-0.1, -0.1, -0.1, -0.1, -0.1],\n",
       "       [-0.1, -0.1, -0.1,  1. , -0.1],\n",
       "       [-0.1, -0.1, -0.1, -0.1, -0.1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False,  True],\n",
       "       [False, False, False, False, False],\n",
       "       [False, False, False, False, False],\n",
       "       [False, False, False, False, False],\n",
       "       [False, False, False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False],\n",
       "       [False,  True, False, False, False],\n",
       "       [False, False, False, False, False],\n",
       "       [False, False, False,  True, False],\n",
       "       [False, False, False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obstacle_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x8606a70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAABN1JREFUeJzt17GRU+kWRtHT742HTPDbIwB1AASlAAiACG4EykIEoAAaD0MOBaaw72QwLQ0Dt3bXWlXXO8aHil3198O6rusAKf/begBwP+FCkHAhSLgQJFwIEi4ECReChAtBwoWgv+45fvv27Tw+Pv6mKf+tnz9/zps3b7aecbPS3tLWmdber1+/zo8fP168uyvcx8fHOZ/P/3rUn/T58+f58OHD1jNuVtpb2jrT2vv09HTTnacyBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQ9rOu6/tPBsiyzLMvMzFwulzkej39k2K/69u3bXC6XrWfc7P3797Pb7baecZPr9ZrZOtPaezgc5nw+v3y43mG/399zvqlPnz6tM5P5TqfT1j/ZzUpb17W199bGPJUhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIejVhrvf72dd18wH93hYX/hfsyzLLMsyMzOXy2WOx+MfGfarrtfr7Ha7rWfcrLS3tHWmtfdwOMz5fH75cL3Dfr+/53xTp9Np6wl3Ke0tbV3X1t5bG3u1T2V4zYQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4Ie1nVd/+lgWZZZlmVmZi6XyxyPxz8y7Fddr9fZ7XZbz7hZae/1ep0vX75sPeNm79+/z/y2h8Nhzufzy4frHfb7/T3nmzqdTltPuEtp7+l0Wmcm85V+21sb81SGIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDh8q+s65r5XqOH9YV/2bIssyzLzMxcLpc5Ho9/ZNivul6vs9vttp5xs9Le0taZ1t7D4TDn8/nlw/UO+/3+nvNNnU6nrSfcpbS3tHVdW3tvbcxTGYKEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReC/nrpYFmWWZZlZmaen5/n6enpt4/6L3z//n3evXu39YyblfaWts609j4/P99097Cu6/qbt2zi6elpzufz1jNuVtpb2jrT2nvrVk9lCBIuBP3/48ePH7ce8bvs9/utJ9yltLe0daa195atr/ZvXHjNPJUhSLgQJFwIEi4ECReC/gZZyHs/li+pQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x84b71b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.grid('on')\n",
    "nrows, ncols =shape\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.imshow(obstacle_mask*-1, cmap='gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = GridWorldMDP(reward_grid=reward_grid,\n",
    "                     obstacle_mask=obstacle_mask,\n",
    "                     terminal_mask=terminal_mask,\n",
    "                     action_probabilities=[\n",
    "                         (-1, 0.1),\n",
    "                         (0, 0.8),\n",
    "                         (1, 0.1),\n",
    "                     ],\n",
    "                     no_action_probability=0.0)\n",
    "mdp_solvers = {'Value Iteration': gw.run_value_iterations,\n",
    "                'Policy Iteration': gw.run_policy_iterations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final result of Value Iteration:\n",
      "[[ 1.  1.  1.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  0.]]\n",
      "[[ 1.45507297  1.59570002  1.72070027  1.85939664  2.        ]\n",
      " [ 1.33006887  2.58286934  1.61113005  1.73456992  1.85939664]\n",
      " [ 1.22049316  1.3439692   1.48441235  1.61113005  1.72070027]\n",
      " [ 1.10935988  1.22049255  1.3439692   2.58286934  1.59570002]\n",
      " [ 0.99820028  1.10935988  1.22049316  1.33006887  1.45507297]]\n",
      "Final result of Policy Iteration:\n",
      "[[ 1.  1.  1.  1.  0.]\n",
      " [ 0.  1.  0.  1.  0.]\n",
      " [ 0.  1.  1.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  0.]]\n",
      "[[ 1.45507298  1.59570002  1.72070027  1.85939664  2.        ]\n",
      " [ 1.3300689   2.58286935  1.61113005  1.73456992  1.85939664]\n",
      " [ 1.22049321  1.34396926  1.48441236  1.61113005  1.72070027]\n",
      " [ 1.10935999  1.22049268  1.34396926  2.58286935  1.59570002]\n",
      " [ 0.99820071  1.1093601   1.22049325  1.33006891  1.45507299]]\n"
     ]
    }
   ],
   "source": [
    "for solver_name, solver_fn in mdp_solvers.items():\n",
    "        print('Final result of {}:'.format(solver_name))\n",
    "        policy_grids, utility_grids = solver_fn(iterations=25, discount=0.5)\n",
    "        print(policy_grids[:, :, -1])\n",
    "        print(utility_grids[:, :, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x85dfa50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "ax = plt.gca()\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "gw.plot_policy(utility_grids[:, :, -1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the best solution starting from the bottom, we should start moving up until we reach the limit, and then we move straight right to reach the goal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
