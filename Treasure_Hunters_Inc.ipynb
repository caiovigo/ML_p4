{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treasure Hunters Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to use reinforcement learning in a grid environment to avoid obstacles and reach the goal. This is done defining cost and reward functions with **Markov Decision Processes (MDP)**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the basic ideas of Reinforcement Learning:\n",
    "\n",
    "## Basic idea:\n",
    "\n",
    "* Receive feedback in the form of rewards\n",
    "* Agent’s utility is defined by the reward function\n",
    "* Must (learn to) act so as to maximize expected rewards "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid World\n",
    "\n",
    "* The agent lives in a grid\n",
    "* Walls block the agent’s path\n",
    "* The agent’s actions do not always go as planned\n",
    "* Small rewards at each step\n",
    "* Big rewards come at the end\n",
    "* Goal: maximize sum of rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of **Markov Decision Processes**.\n",
    "\n",
    "## Markov Decision Processes\n",
    "\n",
    "\n",
    "* An MDP is defined by:\n",
    "    * A set of states s ∈ S\n",
    "    * A set of actions a ∈ A\n",
    "    * A **transition function** T(s,a,s’)\n",
    "    * A **reward function** R(s, a, s’)\n",
    "    * A **start state** (or distribution)\n",
    "    * A **terminal state** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import all the libraries that we will use in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define all the above functions and probabilities. In order to do that, we define a class  called **GridWorldMDP**. With it we can define a function of the dollowing parameters:\n",
    "\n",
    "* reward_grid,\n",
    "* terminal_mask,\n",
    "* obstacle_mask,\n",
    "* action_probabilities,\n",
    "* no_action_probability\n",
    "\n",
    "With them we run a **Markov Decision Processes** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldMDP:\n",
    "\n",
    "    # up, right, down, left\n",
    "    _direction_deltas = [\n",
    "        (-1, 0),\n",
    "        (0, 1),\n",
    "        (1, 0),\n",
    "        (0, -1),\n",
    "    ]\n",
    "    _num_actions = len(_direction_deltas)\n",
    "\n",
    "    def __init__(self,\n",
    "                 reward_grid,\n",
    "                 terminal_mask,\n",
    "                 obstacle_mask,\n",
    "                 action_probabilities,\n",
    "                 no_action_probability):\n",
    "\n",
    "        self._reward_grid = reward_grid\n",
    "        self._terminal_mask = terminal_mask\n",
    "        self._obstacle_mask = obstacle_mask\n",
    "        self._T = self._create_transition_matrix(\n",
    "            action_probabilities,\n",
    "            no_action_probability,\n",
    "            obstacle_mask\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._reward_grid.shape\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return self._reward_grid.size\n",
    "\n",
    "    @property\n",
    "    def reward_grid(self):\n",
    "        return self._reward_grid\n",
    "\n",
    "    def run_value_iterations(self, discount=1.0,\n",
    "                             iterations=10):\n",
    "        utility_grids, policy_grids = self._init_utility_policy_storage(iterations)\n",
    "\n",
    "        utility_grid = np.zeros_like(self._reward_grid)\n",
    "        for i in range(iterations):\n",
    "            utility_grid = self._value_iteration(utility_grid=utility_grid)\n",
    "            policy_grids[:, :, i] = self.best_policy(utility_grid)\n",
    "            utility_grids[:, :, i] = utility_grid\n",
    "        return policy_grids, utility_grids\n",
    "\n",
    "    def run_policy_iterations(self, discount=1.0,\n",
    "                              iterations=10):\n",
    "        utility_grids, policy_grids = self._init_utility_policy_storage(iterations)\n",
    "\n",
    "        policy_grid = np.random.randint(0, self._num_actions,\n",
    "                                        self.shape)\n",
    "        utility_grid = self._reward_grid.copy()\n",
    "\n",
    "        for i in range(iterations):\n",
    "            policy_grid, utility_grid = self._policy_iteration(\n",
    "                policy_grid=policy_grid,\n",
    "                utility_grid=utility_grid\n",
    "            )\n",
    "            policy_grids[:, :, i] = policy_grid\n",
    "            utility_grids[:, :, i] = utility_grid\n",
    "        return policy_grids, utility_grids\n",
    "\n",
    "    def generate_experience(self, current_state_idx, action_idx):\n",
    "        sr, sc = self.grid_indices_to_coordinates(current_state_idx)\n",
    "        next_state_probs = self._T[sr, sc, action_idx, :, :].flatten()\n",
    "\n",
    "        next_state_idx = np.random.choice(np.arange(next_state_probs.size),\n",
    "                                          p=next_state_probs)\n",
    "\n",
    "        return (next_state_idx,\n",
    "                self._reward_grid.flatten()[next_state_idx],\n",
    "                self._terminal_mask.flatten()[next_state_idx])\n",
    "\n",
    "    def grid_indices_to_coordinates(self, indices=None):\n",
    "        if indices is None:\n",
    "            indices = np.arange(self.size)\n",
    "        return np.unravel_index(indices, self.shape)\n",
    "\n",
    "    def grid_coordinates_to_indices(self, coordinates=None):\n",
    "        # Annoyingly, this doesn't work for negative indices.\n",
    "        # The mode='wrap' parameter only works on positive indices.\n",
    "        if coordinates is None:\n",
    "            return np.arange(self.size)\n",
    "        return np.ravel_multi_index(coordinates, self.shape)\n",
    "\n",
    "    def best_policy(self, utility_grid):\n",
    "        M, N = self.shape\n",
    "        return np.argmax((utility_grid.reshape((1, 1, 1, M, N)) * self._T)\n",
    "                         .sum(axis=-1).sum(axis=-1), axis=2)\n",
    "\n",
    "    def _init_utility_policy_storage(self, depth):\n",
    "        M, N = self.shape\n",
    "        utility_grids = np.zeros((M, N, depth))\n",
    "        policy_grids = np.zeros_like(utility_grids)\n",
    "        return utility_grids, policy_grids\n",
    "\n",
    "    def _create_transition_matrix(self,\n",
    "                                  action_probabilities,\n",
    "                                  no_action_probability,\n",
    "                                  obstacle_mask):\n",
    "        M, N = self.shape\n",
    "\n",
    "        T = np.zeros((M, N, self._num_actions, M, N))\n",
    "\n",
    "        r0, c0 = self.grid_indices_to_coordinates()\n",
    "\n",
    "        T[r0, c0, :, r0, c0] += no_action_probability\n",
    "\n",
    "        for action in range(self._num_actions):\n",
    "            for offset, P in action_probabilities:\n",
    "                direction = (action + offset) % self._num_actions\n",
    "\n",
    "                dr, dc = self._direction_deltas[direction]\n",
    "                r1 = np.clip(r0 + dr, 0, M - 1)\n",
    "                c1 = np.clip(c0 + dc, 0, N - 1)\n",
    "\n",
    "                temp_mask = obstacle_mask[r1, c1].flatten()\n",
    "                r1[temp_mask] = r0[temp_mask]\n",
    "                c1[temp_mask] = c0[temp_mask]\n",
    "\n",
    "                T[r0, c0, action, r1, c1] += P\n",
    "\n",
    "        terminal_locs = np.where(self._terminal_mask.flatten())[0]\n",
    "        T[r0[terminal_locs], c0[terminal_locs], :, :, :] = 0\n",
    "        return T\n",
    "\n",
    "    def _value_iteration(self, utility_grid, discount=1.0):\n",
    "        out = np.zeros_like(utility_grid)\n",
    "        M, N = self.shape\n",
    "        for i in range(M):\n",
    "            for j in range(N):\n",
    "                out[i, j] = self._calculate_utility((i, j),\n",
    "                                                    discount,\n",
    "                                                    utility_grid)\n",
    "        return out\n",
    "\n",
    "    def _policy_iteration(self, *, utility_grid,\n",
    "                          policy_grid, discount=1.0):\n",
    "        r, c = self.grid_indices_to_coordinates()\n",
    "\n",
    "        M, N = self.shape\n",
    "\n",
    "        utility_grid = (\n",
    "            self._reward_grid +\n",
    "            discount * ((utility_grid.reshape((1, 1, 1, M, N)) * self._T)\n",
    "                        .sum(axis=-1).sum(axis=-1))[r, c, policy_grid.flatten()]\n",
    "            .reshape(self.shape)\n",
    "        )\n",
    "\n",
    "        utility_grid[self._terminal_mask] = self._reward_grid[self._terminal_mask]\n",
    "\n",
    "        return self.best_policy(utility_grid), utility_grid\n",
    "\n",
    "    def _calculate_utility(self, loc, discount, utility_grid):\n",
    "        if self._terminal_mask[loc]:\n",
    "            return self._reward_grid[loc]\n",
    "        row, col = loc\n",
    "        return np.max(\n",
    "            discount * np.sum(\n",
    "                np.sum(self._T[row, col, :, :, :] * utility_grid,\n",
    "                       axis=-1),\n",
    "                axis=-1)\n",
    "        ) + self._reward_grid[loc]\n",
    "\n",
    "    def plot_policy(self, utility_grid, policy_grid=None):\n",
    "        if policy_grid is None:\n",
    "            policy_grid = self.best_policy(utility_grid)\n",
    "        markers = \"^>v<\"\n",
    "        marker_size = 100 // np.max(policy_grid.shape)\n",
    "        marker_edge_width = marker_size // 10\n",
    "        marker_fill_color = 'w'\n",
    "\n",
    "        no_action_mask = self._terminal_mask | self._obstacle_mask\n",
    "\n",
    "        utility_normalized = (utility_grid - utility_grid.min()) / \\\n",
    "                             (utility_grid.max() - utility_grid.min())\n",
    "\n",
    "        utility_normalized = (255*utility_normalized).astype(np.uint8)\n",
    "\n",
    "        utility_rgb = cv2.applyColorMap(utility_normalized, cv2.COLORMAP_JET)\n",
    "        for i in range(3):\n",
    "            channel = utility_rgb[:, :, i]\n",
    "            channel[self._obstacle_mask] = 0\n",
    "\n",
    "        plt.imshow(utility_rgb[:, :, ::-1], interpolation='none')\n",
    "            \n",
    "        for i, marker in enumerate(markers):\n",
    "            y, x = np.where((policy_grid == i) & np.logical_not(no_action_mask))\n",
    "            plt.plot(x, y, marker, ms=marker_size, mew=marker_edge_width, color=marker_fill_color)\n",
    "\n",
    "        y, x = np.where(self._terminal_mask)\n",
    "        plt.plot(x, y, 'o', ms=marker_size, mew=marker_edge_width, color=marker_fill_color)\n",
    "\n",
    "        tick_step_options = np.array([1, 2, 5, 10, 20, 50, 100])\n",
    "        tick_step = np.max(policy_grid.shape)/8\n",
    "        best_option = np.argmin(np.abs(np.log(tick_step) - np.log(tick_step_options)))\n",
    "        tick_step = tick_step_options[best_option]\n",
    "        plt.xticks(np.arange(0, policy_grid.shape[1] - 0.5, tick_step))\n",
    "        plt.yticks(np.arange(0, policy_grid.shape[0] - 0.5, tick_step))\n",
    "        plt.xlim([-0.5, policy_grid.shape[0]-0.5])\n",
    "        plt.xlim([-0.5, policy_grid.shape[1]-0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our first toy example. To make it easier, let's use a 2x2 grid.\n",
    "\n",
    "The game starts at the left bottom corner.\n",
    "\n",
    "The end is the top right corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting a 3x3 game with One Obstacle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (3, 3)\n",
    "goal = (0, 2)\n",
    "trap = ((1, 1))\n",
    "obstacle = (1, 1)\n",
    "start = (-3, 0)\n",
    "default_reward = -0.1\n",
    "goal_reward = 2\n",
    "trap_reward = -1\n",
    "\n",
    "reward_grid = np.zeros(shape) + default_reward\n",
    "reward_grid[goal] = goal_reward\n",
    "reward_grid[trap] = trap_reward\n",
    "reward_grid[obstacle] = 1\n",
    "\n",
    "terminal_mask = np.zeros_like(reward_grid, dtype=np.bool)\n",
    "terminal_mask[goal] = True\n",
    "terminal_mask[trap] = False\n",
    "\n",
    "obstacle_mask = np.zeros_like(reward_grid, dtype=np.bool)\n",
    "obstacle_mask[1, 1] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we just want to show how the game look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1, -0.1,  2. ],\n",
       "       [-0.1,  1. , -0.1],\n",
       "       [-0.1, -0.1, -0.1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True],\n",
       "       [False, False, False],\n",
       "       [False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False],\n",
       "       [False,  True, False],\n",
       "       [False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obstacle_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x851d3f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAA/hJREFUeJzt17FNK1sUQNHLFxkp5M4ogCnA1VANr4FpBhfgAhwSOEEQQjy/A2M/gezNW0ua7AZHx96aO1fLsiwDSPnv3AMApxMuBAkXgoQLQcKFIOFCkHAhSLgQJFwIuj7l8O3t7VitVj80St/n5+e4ubk59xgXzY4Oe3l5Ge/v71+eOync1Wo1ttvtXw/12202m7Fer889xkWzo8OmaTrqnKsyBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIej6qwPzPI95nscYY+z3+7HZbH56pqzX19fx58+fc49x0e7v7/2HvsNygoeHh1OO/3Oenp6WMYbnwPP8/Hzun+miHduYqzIECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAh6PrcA/wmDw8PY1mWc49x0TabzblH+BW+DHee5zHP8xhjjP1+b/EHfHx82M8X7Oh7XC0nvCKmaRrb7fYn50nbbDZjvV6fe4yLZkeHHduYb1wIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFC0PVXB+Z5HvM8jzHG2O12Y5qmHx+q6u3tbdzd3Z17jItmR4ftdrujzl0ty7L88Cz/jGmaxna7PfcYF82ODjt2P67KECRcCBLuN3p8fDz3CBfPjg47dj++cSHIGxeChAtBwoUg4UKQcCHof8NATbIbU/bVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x84c08b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.grid('on')\n",
    "nrows, ncols =shape\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.imshow(obstacle_mask*-1, cmap='gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = GridWorldMDP(reward_grid=reward_grid,\n",
    "                     obstacle_mask=obstacle_mask,\n",
    "                     terminal_mask=terminal_mask,\n",
    "                     action_probabilities=[\n",
    "                         (-1, 0.1),\n",
    "                         (0, 0.8),\n",
    "                         (1, 0.1),\n",
    "                     ],\n",
    "                     no_action_probability=0.0)\n",
    "mdp_solvers = {'Value Iteration': gw.run_value_iterations,\n",
    "                'Policy Iteration': gw.run_policy_iterations}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we print the results of our policy and utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final result of Value Iteration:\n",
      "[[ 1.  1.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "[[ 1.73437499  1.875       2.        ]\n",
      " [ 1.60937497  2.84843749  1.875     ]\n",
      " [ 1.49826381  1.60937497  1.73437499]]\n",
      "Final result of Policy Iteration:\n",
      "[[ 1.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  1.  0.]]\n",
      "[[ 1.73437499  1.875       2.        ]\n",
      " [ 1.60937497  2.8484375   1.875     ]\n",
      " [ 1.49826385  1.60937499  1.734375  ]]\n"
     ]
    }
   ],
   "source": [
    "for solver_name, solver_fn in mdp_solvers.items():\n",
    "        print('Final result of {}:'.format(solver_name))\n",
    "        policy_grids, utility_grids = solver_fn(iterations=25, discount=0.5)\n",
    "        print(policy_grids[:, :, -1])\n",
    "        print(utility_grids[:, :, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the final result of this toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x84e9570>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "ax = plt.gca()\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "gw.plot_policy(utility_grids[:, :, -1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, there are two possible solutions. Starting from the bottom, we can either start moving right or up and we will avoid the obstacle and reach the goal with the same number of steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting a 5x5 game with Two Obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (5, 5)\n",
    "goal = (0, 4)\n",
    "trap = ((1,1))\n",
    "obstacle1 = (1,1)\n",
    "obstacle2 = (3,3)\n",
    "start = (-4, 0)\n",
    "default_reward = -0.1\n",
    "goal_reward = 2\n",
    "trap_reward = -1\n",
    "\n",
    "reward_grid = np.zeros(shape) + default_reward\n",
    "reward_grid[goal] = goal_reward\n",
    "reward_grid[trap] = trap_reward\n",
    "reward_grid[obstacle1] = 1\n",
    "reward_grid[obstacle2] = 1\n",
    "\n",
    "terminal_mask = np.zeros_like(reward_grid, dtype=np.bool)\n",
    "terminal_mask[goal] = True\n",
    "terminal_mask[trap] = False\n",
    "\n",
    "obstacle_mask = np.zeros_like(reward_grid, dtype=np.bool)\n",
    "obstacle_mask[obstacle1] = True\n",
    "obstacle_mask[obstacle2] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1, -0.1, -0.1, -0.1,  2. ],\n",
       "       [-0.1,  1. , -0.1, -0.1, -0.1],\n",
       "       [-0.1, -0.1, -0.1, -0.1, -0.1],\n",
       "       [-0.1, -0.1, -0.1,  1. , -0.1],\n",
       "       [-0.1, -0.1, -0.1, -0.1, -0.1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False,  True],\n",
       "       [False, False, False, False, False],\n",
       "       [False, False, False, False, False],\n",
       "       [False, False, False, False, False],\n",
       "       [False, False, False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False],\n",
       "       [False,  True, False, False, False],\n",
       "       [False, False, False, False, False],\n",
       "       [False, False, False,  True, False],\n",
       "       [False, False, False, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obstacle_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x8837c30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAABN1JREFUeJzt17GRU+kWRtHT742HTPDbIwB1AASlAAiACG4EykIEoAAaD0MOBaaw72QwLQ0Dt3bXWlXXO8aHil3198O6rusAKf/begBwP+FCkHAhSLgQJFwIEi4ECReChAtBwoWgv+45fvv27Tw+Pv6mKf+tnz9/zps3b7aecbPS3tLWmdber1+/zo8fP168uyvcx8fHOZ/P/3rUn/T58+f58OHD1jNuVtpb2jrT2vv09HTTnacyBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQ9rOu6/tPBsiyzLMvMzFwulzkej39k2K/69u3bXC6XrWfc7P3797Pb7baecZPr9ZrZOtPaezgc5nw+v3y43mG/399zvqlPnz6tM5P5TqfT1j/ZzUpb17W199bGPJUhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIejVhrvf72dd18wH93hYX/hfsyzLLMsyMzOXy2WOx+MfGfarrtfr7Ha7rWfcrLS3tHWmtfdwOMz5fH75cL3Dfr+/53xTp9Np6wl3Ke0tbV3X1t5bG3u1T2V4zYQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4Ie1nVd/+lgWZZZlmVmZi6XyxyPxz8y7Fddr9fZ7XZbz7hZae/1ep0vX75sPeNm79+/z/y2h8Nhzufzy4frHfb7/T3nmzqdTltPuEtp7+l0Wmcm85V+21sb81SGIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDh8q+s65r5XqOH9YV/2bIssyzLzMxcLpc5Ho9/ZNivul6vs9vttp5xs9Le0taZ1t7D4TDn8/nlw/UO+/3+nvNNnU6nrSfcpbS3tHVdW3tvbcxTGYKEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReC/nrpYFmWWZZlZmaen5/n6enpt4/6L3z//n3evXu39YyblfaWts609j4/P99097Cu6/qbt2zi6elpzufz1jNuVtpb2jrT2nvrVk9lCBIuBP3/48ePH7ce8bvs9/utJ9yltLe0daa195atr/ZvXHjNPJUhSLgQJFwIEi4ECReC/gZZyHs/li+pQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x84c0230>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.grid('on')\n",
    "nrows, ncols =shape\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.imshow(obstacle_mask*-1, cmap='gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = GridWorldMDP(reward_grid=reward_grid,\n",
    "                     obstacle_mask=obstacle_mask,\n",
    "                     terminal_mask=terminal_mask,\n",
    "                     action_probabilities=[\n",
    "                         (-1, 0.1),\n",
    "                         (0, 0.8),\n",
    "                         (1, 0.1),\n",
    "                     ],\n",
    "                     no_action_probability=0.0)\n",
    "mdp_solvers = {'Value Iteration': gw.run_value_iterations,\n",
    "                'Policy Iteration': gw.run_policy_iterations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final result of Value Iteration:\n",
      "[[ 1.  1.  1.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 1.  1.  1.  1.  0.]]\n",
      "[[ 1.45507297  1.59570002  1.72070027  1.85939664  2.        ]\n",
      " [ 1.33006887  2.58286934  1.61113005  1.73456992  1.85939664]\n",
      " [ 1.22049316  1.3439692   1.48441235  1.61113005  1.72070027]\n",
      " [ 1.10935988  1.22049255  1.3439692   2.58286934  1.59570002]\n",
      " [ 0.99820028  1.10935988  1.22049316  1.33006887  1.45507297]]\n",
      "Final result of Policy Iteration:\n",
      "[[ 1.  1.  1.  1.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1.  1.  0.]]\n",
      "[[ 1.45507299  1.59570002  1.72070027  1.85939664  2.        ]\n",
      " [ 1.33006891  2.58286935  1.61113005  1.73456992  1.85939664]\n",
      " [ 1.22049325  1.34396926  1.48441236  1.61113005  1.72070027]\n",
      " [ 1.1093601   1.22049268  1.34396926  2.58286935  1.59570002]\n",
      " [ 0.99820071  1.10935999  1.22049321  1.3300689   1.45507298]]\n"
     ]
    }
   ],
   "source": [
    "for solver_name, solver_fn in mdp_solvers.items():\n",
    "        print('Final result of {}:'.format(solver_name))\n",
    "        policy_grids, utility_grids = solver_fn(iterations=25, discount=0.5)\n",
    "        print(policy_grids[:, :, -1])\n",
    "        print(utility_grids[:, :, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x881c510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "ax = plt.gca()\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "gw.plot_policy(utility_grids[:, :, -1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the best solution starting from the bottom, we should start moving up until we reach the limit, and then we move straight right to reach the goal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
